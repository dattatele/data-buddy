{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_warp.connectors.file_connector import FileConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_file = DataGenerator(num_rows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>int_field</th>\n",
       "      <th>float_field</th>\n",
       "      <th>text_field</th>\n",
       "      <th>string_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>483 Cedar Ave, Phoenix</td>\n",
       "      <td>2025-02-01 17:43:16</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>8211</td>\n",
       "      <td>466.880282</td>\n",
       "      <td>adipiscing sit amet sit amet</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>Bob Brown</td>\n",
       "      <td>8239 Maple Ave, Phoenix</td>\n",
       "      <td>2012-05-31 05:26:47</td>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>9335</td>\n",
       "      <td>628.202743</td>\n",
       "      <td>consectetur adipiscing amet consectetur consec...</td>\n",
       "      <td>dolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>Charlie Garcia</td>\n",
       "      <td>3599 Oak St, Houston</td>\n",
       "      <td>2007-03-21 10:42:56</td>\n",
       "      <td>2007-03-21</td>\n",
       "      <td>8565</td>\n",
       "      <td>312.719842</td>\n",
       "      <td>amet dolor consectetur elit dolor</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>Alice Davis</td>\n",
       "      <td>1828 Pine St, Chicago</td>\n",
       "      <td>2019-07-06 15:58:48</td>\n",
       "      <td>2019-07-06</td>\n",
       "      <td>5091</td>\n",
       "      <td>669.503191</td>\n",
       "      <td>dolor ipsum sit ipsum ipsum</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ef2d127de37b942baad06145e54b0c619a1f22327b2ebb...</td>\n",
       "      <td>Alice Smith</td>\n",
       "      <td>7918 Cedar Ave, New York</td>\n",
       "      <td>2015-09-18 11:44:26</td>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>7791</td>\n",
       "      <td>48.412922</td>\n",
       "      <td>adipiscing consectetur lorem elit dolor</td>\n",
       "      <td>sit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            hash_id            name  \\\n",
       "0   1  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...     Alice Brown   \n",
       "1   2  d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...       Bob Brown   \n",
       "2   3  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...  Charlie Garcia   \n",
       "3   4  4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...     Alice Davis   \n",
       "4   5  ef2d127de37b942baad06145e54b0c619a1f22327b2ebb...     Alice Smith   \n",
       "\n",
       "                    address            datetime        date  int_field  \\\n",
       "0    483 Cedar Ave, Phoenix 2025-02-01 17:43:16  2025-02-01       8211   \n",
       "1   8239 Maple Ave, Phoenix 2012-05-31 05:26:47  2012-05-31       9335   \n",
       "2      3599 Oak St, Houston 2007-03-21 10:42:56  2007-03-21       8565   \n",
       "3     1828 Pine St, Chicago 2019-07-06 15:58:48  2019-07-06       5091   \n",
       "4  7918 Cedar Ave, New York 2015-09-18 11:44:26  2015-09-18       7791   \n",
       "\n",
       "   float_field                                         text_field string_field  \n",
       "0   466.880282                       adipiscing sit amet sit amet         amet  \n",
       "1   628.202743  consectetur adipiscing amet consectetur consec...        dolor  \n",
       "2   312.719842                  amet dolor consectetur elit dolor         amet  \n",
       "3   669.503191                        dolor ipsum sit ipsum ipsum         amet  \n",
       "4    48.412922            adipiscing consectetur lorem elit dolor          sit  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the #default file format\n",
    "FileConnector(generator_file.to_csv_file()).fetch().head()\n",
    "FileConnector(generator_file.to_excel_file()).fetch().head()\n",
    "FileConnector(generator_file.to_json_file()).fetch().head()\n",
    "FileConnector(generator_file.to_parquet_file()).fetch().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>int_field</th>\n",
       "      <th>float_field</th>\n",
       "      <th>text_field</th>\n",
       "      <th>string_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>483 Cedar Ave, Phoenix</td>\n",
       "      <td>2025-02-01 17:43:16</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>8211</td>\n",
       "      <td>466.880282</td>\n",
       "      <td>adipiscing sit amet sit amet</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>Bob Brown</td>\n",
       "      <td>8239 Maple Ave, Phoenix</td>\n",
       "      <td>2012-05-31 05:26:47</td>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>9335</td>\n",
       "      <td>628.202743</td>\n",
       "      <td>consectetur adipiscing amet consectetur consec...</td>\n",
       "      <td>dolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>Charlie Garcia</td>\n",
       "      <td>3599 Oak St, Houston</td>\n",
       "      <td>2007-03-21 10:42:56</td>\n",
       "      <td>2007-03-21</td>\n",
       "      <td>8565</td>\n",
       "      <td>312.719842</td>\n",
       "      <td>amet dolor consectetur elit dolor</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>Alice Davis</td>\n",
       "      <td>1828 Pine St, Chicago</td>\n",
       "      <td>2019-07-06 15:58:48</td>\n",
       "      <td>2019-07-06</td>\n",
       "      <td>5091</td>\n",
       "      <td>669.503191</td>\n",
       "      <td>dolor ipsum sit ipsum ipsum</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ef2d127de37b942baad06145e54b0c619a1f22327b2ebb...</td>\n",
       "      <td>Alice Smith</td>\n",
       "      <td>7918 Cedar Ave, New York</td>\n",
       "      <td>2015-09-18 11:44:26</td>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>7791</td>\n",
       "      <td>48.412922</td>\n",
       "      <td>adipiscing consectetur lorem elit dolor</td>\n",
       "      <td>sit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            hash_id            name  \\\n",
       "0   1  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...     Alice Brown   \n",
       "1   2  d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...       Bob Brown   \n",
       "2   3  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...  Charlie Garcia   \n",
       "3   4  4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...     Alice Davis   \n",
       "4   5  ef2d127de37b942baad06145e54b0c619a1f22327b2ebb...     Alice Smith   \n",
       "\n",
       "                    address            datetime        date  int_field  \\\n",
       "0    483 Cedar Ave, Phoenix 2025-02-01 17:43:16  2025-02-01       8211   \n",
       "1   8239 Maple Ave, Phoenix 2012-05-31 05:26:47  2012-05-31       9335   \n",
       "2      3599 Oak St, Houston 2007-03-21 10:42:56  2007-03-21       8565   \n",
       "3     1828 Pine St, Chicago 2019-07-06 15:58:48  2019-07-06       5091   \n",
       "4  7918 Cedar Ave, New York 2015-09-18 11:44:26  2015-09-18       7791   \n",
       "\n",
       "   float_field                                         text_field string_field  \n",
       "0   466.880282                       adipiscing sit amet sit amet         amet  \n",
       "1   628.202743  consectetur adipiscing amet consectetur consec...        dolor  \n",
       "2   312.719842                  amet dolor consectetur elit dolor         amet  \n",
       "3   669.503191                        dolor ipsum sit ipsum ipsum         amet  \n",
       "4    48.412922            adipiscing consectetur lorem elit dolor          sit  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch_batch #default file format\n",
    "FileConnector(generator_file.to_csv_file()).fetch_batch()[0].head()\n",
    "FileConnector(generator_file.to_json_file()).fetch_batch()[0].head()\n",
    "FileConnector(generator_file.to_parquet_file()).fetch_batch()[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileConnector(generator_file.to_parquet_file()).fetch_batch()[0].head()[\"hash_id\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_warp.connectors.utils.StreamingBatchIterator at 0x1c9decc5760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileConnector(generator_file.to_json_file(), reader=\"builtin\").fetch_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connector = FileConnector(file_path=generator_file.to_json_file(), file_type=\"json\", source=\"local\", reader=\"builtin\")\n",
    "batches = connector.fetch_batch()\n",
    "flatten_batch = batches.flatten_to_list()\n",
    "type(flatten_batch)\n",
    "len(flatten_batch)\n",
    "isinstance(flatten_batch, list)\n",
    "#batches.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>int_field</th>\n",
       "      <th>float_field</th>\n",
       "      <th>text_field</th>\n",
       "      <th>string_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>483 Cedar Ave, Phoenix</td>\n",
       "      <td>2025-02-01T17:43:16.000</td>\n",
       "      <td>2025-02-01T00:00:00.000</td>\n",
       "      <td>8211</td>\n",
       "      <td>466.880282</td>\n",
       "      <td>adipiscing sit amet sit amet</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>Bob Brown</td>\n",
       "      <td>8239 Maple Ave, Phoenix</td>\n",
       "      <td>2012-05-31T05:26:47.000</td>\n",
       "      <td>2012-05-31T00:00:00.000</td>\n",
       "      <td>9335</td>\n",
       "      <td>628.202743</td>\n",
       "      <td>consectetur adipiscing amet consectetur consec...</td>\n",
       "      <td>dolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>Charlie Garcia</td>\n",
       "      <td>3599 Oak St, Houston</td>\n",
       "      <td>2007-03-21T10:42:56.000</td>\n",
       "      <td>2007-03-21T00:00:00.000</td>\n",
       "      <td>8565</td>\n",
       "      <td>312.719842</td>\n",
       "      <td>amet dolor consectetur elit dolor</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>Alice Davis</td>\n",
       "      <td>1828 Pine St, Chicago</td>\n",
       "      <td>2019-07-06T15:58:48.000</td>\n",
       "      <td>2019-07-06T00:00:00.000</td>\n",
       "      <td>5091</td>\n",
       "      <td>669.503191</td>\n",
       "      <td>dolor ipsum sit ipsum ipsum</td>\n",
       "      <td>amet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ef2d127de37b942baad06145e54b0c619a1f22327b2ebb...</td>\n",
       "      <td>Alice Smith</td>\n",
       "      <td>7918 Cedar Ave, New York</td>\n",
       "      <td>2015-09-18T11:44:26.000</td>\n",
       "      <td>2015-09-18T00:00:00.000</td>\n",
       "      <td>7791</td>\n",
       "      <td>48.412922</td>\n",
       "      <td>adipiscing consectetur lorem elit dolor</td>\n",
       "      <td>sit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            hash_id            name  \\\n",
       "0   1  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...     Alice Brown   \n",
       "1   2  d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...       Bob Brown   \n",
       "2   3  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...  Charlie Garcia   \n",
       "3   4  4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...     Alice Davis   \n",
       "4   5  ef2d127de37b942baad06145e54b0c619a1f22327b2ebb...     Alice Smith   \n",
       "\n",
       "                    address                 datetime                     date  \\\n",
       "0    483 Cedar Ave, Phoenix  2025-02-01T17:43:16.000  2025-02-01T00:00:00.000   \n",
       "1   8239 Maple Ave, Phoenix  2012-05-31T05:26:47.000  2012-05-31T00:00:00.000   \n",
       "2      3599 Oak St, Houston  2007-03-21T10:42:56.000  2007-03-21T00:00:00.000   \n",
       "3     1828 Pine St, Chicago  2019-07-06T15:58:48.000  2019-07-06T00:00:00.000   \n",
       "4  7918 Cedar Ave, New York  2015-09-18T11:44:26.000  2015-09-18T00:00:00.000   \n",
       "\n",
       "   int_field  float_field                                         text_field  \\\n",
       "0       8211   466.880282                       adipiscing sit amet sit amet   \n",
       "1       9335   628.202743  consectetur adipiscing amet consectetur consec...   \n",
       "2       8565   312.719842                  amet dolor consectetur elit dolor   \n",
       "3       5091   669.503191                        dolor ipsum sit ipsum ipsum   \n",
       "4       7791    48.412922            adipiscing consectetur lorem elit dolor   \n",
       "\n",
       "  string_field  \n",
       "0         amet  \n",
       "1        dolor  \n",
       "2         amet  \n",
       "3         amet  \n",
       "4          sit  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch_batch #built-in file format\n",
    "FileConnector(generator_file.to_csv_file(), reader=\"builtin\").fetch_batch()\n",
    "#json has various additional methods to deal with large files and useful for ad-hoc filter\n",
    "#search\n",
    "list(FileConnector(generator_file.to_json_file(), reader=\"builtin\").fetch_batch().search(lambda rec: rec.get(\"hash_id\")==\"6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b\"))\n",
    "#filter\n",
    "filtered = FileConnector(generator_file.to_json_file(), reader=\"builtin\").fetch_batch().filter_batches(lambda rec: rec[0].get(\"int_field\") < 8516)\n",
    "# print(\"filtered\", filtered.next())\n",
    "# Map batches:\n",
    "mapped =  FileConnector(generator_file.to_json_file(), reader=\"builtin\").fetch_batch().map_batches(\n",
    "    lambda batch: [rec for rec in batch if rec.get(\"date\") < \"2002-07-06\"]\n",
    ")\n",
    "# for batch in mapped:\n",
    "#     print(\"Mapped batch:\", batch)\n",
    "\n",
    "FileConnector(generator_file.to_json_file(), reader=\"builtin\").fetch_batch().to_dataframe().head()\n",
    "\n",
    "# FileConnector(generator_file.to_parquet_file(), reader=\"builtin\").fetch_batch()[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "def csv_file():\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False) as f:\n",
    "        df = pd.DataFrame({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n",
    "        df.to_csv(f.name, index=False)\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_buddy.connectors.file_connector.FileConnector at 0x18b44dc9c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileConnector(file_path=str(csv_file()), source=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  Alice   25\n",
       "1    Bob   30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileConnector(file_path=str(csv_file()), source=\"local\").fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import json\n",
    "\n",
    "def json_file():\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False) as f:\n",
    "        data = {\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]}\n",
    "        with open(f.name, 'w') as json_f:\n",
    "            json.dump(data, json_f)\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\datta\\\\AppData\\\\Local\\\\Temp\\\\tmp8qnv5w1v.json'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data\n",
    "assert isinstance(json_data, pd.DataFrame)\n",
    "assert json_data['name'] == [\"Alice\", \"Bob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alice', 'Bob'], dtype='<U5')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([\"Alice\", \"Bob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Alice\n",
       "1      Bob\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileConnector().download_file('https://www.kaggle.com/c/3136/download-all', 'data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import click\n",
    "\n",
    "def generate_large_dataset(\n",
    "    num_rows: int,\n",
    "    num_files: int,\n",
    "    output_dir: str,\n",
    "    file_format: str = \"parquet\"\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate large test datasets.\"\"\"\n",
    "    file_paths = []\n",
    "    rows_per_file = num_rows // num_files\n",
    "    \n",
    "    with click.progressbar(\n",
    "        range(num_files),\n",
    "        label=f'Generating {num_files} {file_format} files',\n",
    "        item_show_func=lambda x: f'File {x}' if x is not None else ''\n",
    "    ) as bar:\n",
    "        for i in bar:\n",
    "            df = pd.DataFrame({\n",
    "                'id': range(i * rows_per_file, (i + 1) * rows_per_file),\n",
    "                'value': np.random.randn(rows_per_file),\n",
    "                'category': np.random.choice(['A', 'B', 'C'], rows_per_file),\n",
    "                'timestamp': pd.date_range(start='2024-01-01', periods=rows_per_file, freq='S')\n",
    "            })\n",
    "            \n",
    "            file_path = Path(output_dir) / f'data_{i}.{file_format}'\n",
    "            if file_format == 'parquet':\n",
    "                df.to_parquet(file_path, engine='pyarrow', compression='snappy')\n",
    "            elif file_format == 'csv':\n",
    "                df.to_csv(file_path, index=False)\n",
    "            elif file_format == 'json':\n",
    "                df.to_json(file_path, orient='records', lines=True)\n",
    "                \n",
    "            file_paths.append(str(file_path))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from typing import List, Generator\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def test_data_dir() -> Generator[str, None, None]:\n",
    "    \"\"\"Create a temporary directory for test data.\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    yield temp_dir\n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import logging\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class FileSource(ABC):\n",
    "    \"\"\"Abstract base class for different file sources.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fetch(self, file_path: str, **kwargs: Any) -> Union[io.BytesIO, str]:\n",
    "        \"\"\"Fetch data from the specified source.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def stream(self, file_path: str, chunk_size: int = 1024 * 1024, **kwargs: Any) -> Any:\n",
    "        \"\"\"Stream data from the specified source in chunks.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LocalFileSource(FileSource):\n",
    "    \"\"\"Fetch data from local file storage.\"\"\"\n",
    "\n",
    "    def fetch(self, file_path: str, **kwargs: Any) -> Union[io.BytesIO, str]:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "    def stream(self, file_path: str, chunk_size: int = 1024 * 1024, **kwargs: Any) -> Any:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            while chunk := file.read(chunk_size):\n",
    "                yield chunk\n",
    "\n",
    "def get_source_handler(source: str, **kwargs):\n",
    "    \"\"\"Get the appropriate file source handler.\"\"\"\n",
    "    handlers = {\n",
    "        \"local\": LocalFileSource()\n",
    "    }\n",
    "    if source not in handlers:\n",
    "        raise ValueError(f\"Unsupported file source: {source}\")\n",
    "    return handlers[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing json batches\n",
      "Generating 1 json files\n",
      "file_obj:  C:\\Users\\datta\\AppData\\Local\\Temp\\tmp6svqit7r\\data_0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\datta\\AppData\\Local\\Temp\\ipykernel_59916\\1476258736.py:25: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  'timestamp': pd.date_range(start='2024-01-01', periods=rows_per_file, freq='S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id     value category           timestamp\n",
      "0          0 -1.987626        A 2024-01-01 00:00:00\n",
      "1          1 -2.306084        C 2024-01-01 00:00:01\n",
      "2          2 -1.568017        B 2024-01-01 00:00:02\n",
      "3          3  0.070322        C 2024-01-01 00:00:03\n",
      "4          4 -0.637974        B 2024-01-01 00:00:04\n",
      "...      ...       ...      ...                 ...\n",
      "99995  99995 -0.681565        A 2024-01-02 03:46:35\n",
      "99996  99996 -0.286041        C 2024-01-02 03:46:36\n",
      "99997  99997  0.539834        A 2024-01-02 03:46:37\n",
      "99998  99998 -0.221162        B 2024-01-02 03:46:38\n",
      "99999  99999 -0.198825        B 2024-01-02 03:46:39\n",
      "\n",
      "[100000 rows x 4 columns]\n",
      "[          id     value category           timestamp\n",
      "0          0 -1.987626        A 2024-01-01 00:00:00\n",
      "1          1 -2.306084        C 2024-01-01 00:00:01\n",
      "2          2 -1.568017        B 2024-01-01 00:00:02\n",
      "3          3  0.070322        C 2024-01-01 00:00:03\n",
      "4          4 -0.637974        B 2024-01-01 00:00:04\n",
      "...      ...       ...      ...                 ...\n",
      "99995  99995 -0.681565        A 2024-01-02 03:46:35\n",
      "99996  99996 -0.286041        C 2024-01-02 03:46:36\n",
      "99997  99997  0.539834        A 2024-01-02 03:46:37\n",
      "99998  99998 -0.221162        B 2024-01-02 03:46:38\n",
      "99999  99999 -0.198825        B 2024-01-02 03:46:39\n",
      "\n",
      "[100000 rows x 4 columns]]\n"
     ]
    }
   ],
   "source": [
    "total_rows = 0\n",
    "batch_sizes = []\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import click\n",
    "from data_buddy.connectors.file_connector import FileConnector\n",
    "\n",
    "with click.progressbar(length=100000,label='Processing json batches') as bar:\n",
    "    with test_data_dir() as temp_dir:\n",
    "        file_paths = generate_large_dataset(num_rows=100000,num_files=1,output_dir=temp_dir,file_format= \"json\")\n",
    "        source_handler = get_source_handler(\"local\")\n",
    "        file_obj = source_handler.fetch(file_paths[0])\n",
    "        print(\"file_obj: \", file_obj)\n",
    "        print(pd.read_json(file_obj, lines=True))\n",
    "\n",
    "        # connector = FileConnector(file_paths[0], file_type=\"parquet\")\n",
    "        connector = FileConnector(file_paths[0], file_type='json')\n",
    "        print(connector.fetch_batch(batch_size=100000))\n",
    "        for batch in connector.fetch_batch(batch_size=100000):\n",
    "            total_rows += len(batch)\n",
    "            batch_sizes.append(len(batch))\n",
    "            bar.update(len(batch))\n",
    "\n",
    "# assert total_rows == 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = FileConnector(file_paths[0], file_type=\"parquet\")\n",
    "total_rows = 0\n",
    "batch_sizes = []\n",
    "\n",
    "with click.progressbar(\n",
    "    length=100000,\n",
    "    label='Processing parquet batches'\n",
    ") as bar:\n",
    "    for batch in connector.fetch_batch(batch_size=100000):\n",
    "        total_rows += len(batch)\n",
    "        batch_sizes.append(len(batch))\n",
    "        bar.update(len(batch))\n",
    "\n",
    "assert total_rows == 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "def json_file_array():\n",
    "    # This JSON file is a JSON array.\n",
    "    data = [{\"a\": 1}, {\"a\": 2}]\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode=\"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        connector = FileConnector(file_path=f.name, file_type=\"json\", source=\"local\", reader=\"builtin\")\n",
    "        batches = connector.fetch_batch(batch_size=1)\n",
    "    #os.unlink(f.name)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  1\n",
       "1  2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batches = json_file_array()\n",
    "batches.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mjson_file_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 10\u001b[0m     connector \u001b[38;5;241m=\u001b[39m FileConnector(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(json_file_array())[\u001b[38;5;241m0\u001b[39m], file_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, reader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(f\u001b[38;5;241m.\u001b[39mname)\n",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36mjson_file_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m}]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, f)\n",
      "File \u001b[1;32mc:\\Users\\datta\\anaconda3\\Lib\\tempfile.py:564\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors, delete_on_close)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create and return a temporary file.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m'prefix', 'suffix', 'dir' -- as for mkstemp.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03mWindows can delete the file even in this case.\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 564\u001b[0m prefix, suffix, \u001b[38;5;28mdir\u001b[39m, output_type \u001b[38;5;241m=\u001b[39m _sanitize_params(prefix, suffix, \u001b[38;5;28mdir\u001b[39m)\n\u001b[0;32m    566\u001b[0m flags \u001b[38;5;241m=\u001b[39m _bin_openflags\n",
      "File \u001b[1;32mc:\\Users\\datta\\anaconda3\\Lib\\tempfile.py:116\u001b[0m, in \u001b[0;36m_sanitize_params\u001b[1;34m(prefix, suffix, dir)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Common parameter processing for most APIs in this module.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m output_type \u001b[38;5;241m=\u001b[39m _infer_return_type(prefix, suffix, \u001b[38;5;28mdir\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\datta\\anaconda3\\Lib\\tempfile.py:92\u001b[0m, in \u001b[0;36m_infer_return_type\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, _os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m     93\u001b[0m     arg \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mfspath(arg)\n",
      "File \u001b[1;32m<frozen abc>:119\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m connector \u001b[38;5;241m=\u001b[39m FileConnector(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(json_file_array())[\u001b[38;5;241m0\u001b[39m], file_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, reader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m batches \u001b[38;5;241m=\u001b[39m connector\u001b[38;5;241m.\u001b[39mfetch_batch(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mjson_file_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, f)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 10\u001b[0m     connector \u001b[38;5;241m=\u001b[39m FileConnector(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(json_file_array())[\u001b[38;5;241m0\u001b[39m], file_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, reader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(f\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connector\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mjson_file_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, f)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 10\u001b[0m     connector \u001b[38;5;241m=\u001b[39m FileConnector(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(json_file_array())[\u001b[38;5;241m0\u001b[39m], file_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, reader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(f\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connector\n",
      "    \u001b[1;31m[... skipping similar frames: json_file_array at line 10 (1485 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mjson_file_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, f)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 10\u001b[0m     connector \u001b[38;5;241m=\u001b[39m FileConnector(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(json_file_array())[\u001b[38;5;241m0\u001b[39m], file_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, reader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(f\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connector\n",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36mjson_file_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson_file_array\u001b[39m():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# This JSON file is a JSON array.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m}]\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, f)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\datta\\anaconda3\\Lib\\tempfile.py:517\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__exit__\u001b[1;34m(self, exc, value, tb)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc, value, tb):\n\u001b[1;32m--> 517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(exc, value, tb)\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closer\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "\n",
    "connector = FileConnector(file_path=list(json_file_array())[0], file_type=\"json\", source=\"local\", reader=\"builtin\")\n",
    "batches = connector.fetch_batch(batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
